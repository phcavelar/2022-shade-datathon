{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3D classification based on DenseNet\n",
    "\n",
    "Based on a tutorial by the MONAI Consortium: https://github.com/Project-MONAI/tutorials/blob/main/3d_classification/densenet_training_array.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import glob\n",
    "import sys\n",
    "import shutil\n",
    "import tempfile\n",
    "import datetime\n",
    "import socket\n",
    "import functools\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, balanced_accuracy_score, roc_auc_score\n",
    "\n",
    "import monai\n",
    "from monai.apps import download_and_extract\n",
    "from monai.config import print_config\n",
    "from monai.data import DataLoader, ImageDataset\n",
    "from monai.transforms import (\n",
    "    EnsureChannelFirst,\n",
    "    Compose,\n",
    "    RandRotate90,\n",
    "    Resize,\n",
    "    ScaleIntensity,\n",
    ")\n",
    "\n",
    "pin_memory = torch.cuda.is_available()\n",
    "torch.backends.cudnn.benchmark = False #torch.cuda.is_available() # Set this to true if the code fails\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "print_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions to get predictions/confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predicted_values(model, loader):\n",
    "    # Iterates through a dataloader and gets the raw predictions and labels\n",
    "    t_model_outputs = []\n",
    "    t_test_labels = []\n",
    "    for test_data in loader:\n",
    "        test_images, test_labels = test_data[0].to(device), test_data[1].to(device)\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(test_images)\n",
    "            t_model_outputs.append(val_outputs.cpu().detach().numpy())\n",
    "            t_test_labels.append(test_labels.cpu().detach().numpy())\n",
    "    conf_model_outputs = np.concatenate(t_model_outputs)\n",
    "    conf_test_labels = np.concatenate(t_test_labels)\n",
    "    return conf_model_outputs, conf_test_labels\n",
    "\n",
    "def get_cm(conf_model_outputs, conf_test_labels, num_classes):\n",
    "    # Get a (count) confusion matrix based on the class predictions and true labels\n",
    "    conf_matrix = np.zeros((num_classes,num_classes))\n",
    "    for i in range(num_classes):\n",
    "        in_class_i = conf_test_labels==i\n",
    "        for j in range(num_classes):\n",
    "            in_class_i_predicted_in_class_j = sum(conf_model_outputs[in_class_i]==j)\n",
    "            conf_matrix[i,j] = in_class_i_predicted_in_class_j\n",
    "    return conf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = os.path.expanduser(os.path.expandvars(\"~/data/medicaldecathlon/\"))\n",
    "data_dir = os.path.join(root_dir, \"Task10_Colon\")\n",
    "train_dataset_frailty_path = os.path.join(data_dir,\"train_clean.csv\")\n",
    "test_dataset_frailty_path = os.path.join(data_dir,\"test_clean.csv\")\n",
    "print(root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get list of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = sorted(\n",
    "    glob.glob(os.path.join(data_dir, \"imagesTr\", \"*.nii.gz\")))\n",
    "train_images[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_paths = sorted(\n",
    "    glob.glob(os.path.join(data_dir, \"imagesTs\", \"*.nii.gz\")))\n",
    "test_image_paths[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get training and testing labels. You should move these from this repo's `data` folder to the same folder as that contain the image folders above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which columns should be forced to be integer columns\n",
    "int_cols = [\"Skeletal Muscle Wasting\",\"Fat Excess\",\"Bone density\",\"Aortic Calcium\",\"Liver fat\",\"Pancreatic fat\",\"Total Score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels = pd.read_csv(train_dataset_frailty_path, index_col=\"PatientID\").dropna().astype({col:int for col in int_cols})\n",
    "df_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels_test = pd.read_csv(test_dataset_frailty_path, index_col=\"PatientID\").dropna().astype({col:int for col in int_cols})\n",
    "df_labels_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change values in the dataframes from string to class ids (Not used here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels.loc[df_labels[\"Risk Category\"]==\"LOW\",\"Risk Category\"] = 0\n",
    "df_labels.loc[df_labels[\"Risk Category\"]==\"MEDIUM\",\"Risk Category\"] = 1\n",
    "df_labels.loc[df_labels[\"Risk Category\"]==\"HIGH\",\"Risk Category\"] = 2\n",
    "\n",
    "df_labels_test.loc[df_labels_test[\"Risk Category\"]==\"LOW\",\"Risk Category\"] = 0\n",
    "df_labels_test.loc[df_labels_test[\"Risk Category\"]==\"MEDIUM\",\"Risk Category\"] = 1\n",
    "df_labels_test.loc[df_labels_test[\"Risk Category\"]==\"HIGH\",\"Risk Category\"] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_id_from_filepath(fpath):\n",
    "    # Gets patient ID from its filepath\n",
    "    return int(os.path.basename(fpath).split(\"_\")[1].split(\".\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_types = [\"Skeletal Muscle Wasting\", \"Fat Excess\", \"Bone density\", \"Aortic Calcium\", \"Liver fat\", \"Pancreatic fat\"]\n",
    "label_type_weights = [2,1,2,1,1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_SLICES = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dicts = [\n",
    "    {\"image\": image_name, \"label\": df_labels.loc[patient_id,label_types].astype(int)}\n",
    "    for image_name,patient_id in zip(train_images,map(get_id_from_filepath,train_images))\n",
    "    if patient_id in df_labels.index and nib.load(image_name).get_fdata().shape[2]>=MIN_SLICES\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_dicts = [\n",
    "    {\"image\": image_name, \"label\": df_labels_test.loc[patient_id,label_types].astype(int)}\n",
    "    for image_name,patient_id in zip(test_image_paths,map(get_id_from_filepath,test_image_paths))\n",
    "    if patient_id in df_labels_test.index and nib.load(image_name).get_fdata().shape[2]>=MIN_SLICES\n",
    "]\n",
    "len(test_data_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IXI dataset as a demo, downloadable from https://brain-development.org/ixi-dataset/\n",
    "images = np.array([d[\"image\"] for d in data_dicts])\n",
    "# 2 binary labels for gender classification: man or woman\n",
    "labels = np.array([d[\"label\"] for d in data_dicts])\n",
    "\n",
    "test_images = np.array([d[\"image\"] for d in test_data_dicts])\n",
    "# 2 binary labels for gender classification: man or woman\n",
    "test_labels = np.array([d[\"label\"] for d in test_data_dicts])\n",
    "\n",
    "\n",
    "# Represent labels in one-hot format for binary classifier training,\n",
    "# BCEWithLogitsLoss requires target to have same shape as input\n",
    "#labels = torch.nn.functional.one_hot(torch.as_tensor(labels)).float()\n",
    "num_labels = [int(max(labels[:,l]))+1 for l in range(labels.shape[1])] if isinstance(labels, np.ndarray) and len(labels.shape)>1 else int(max(labels))+1\n",
    "num_labels_cumsum = np.cumsum(num_labels).tolist()\n",
    "num_labels, num_labels_cumsum, labels[:5], test_labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes = [nib.load(datapoint[\"image\"]).get_fdata().shape for datapoint in data_dicts]\n",
    "np_shapes = np.stack(shapes)\n",
    "minshapes = np.min(np_shapes, axis=0)\n",
    "minshapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_crop = 96\n",
    "for newcrop in range(original_crop,2,-1):\n",
    "    if newcrop <= min(minshapes):\n",
    "        crop_shapes = tuple([newcrop]*3)\n",
    "        break\n",
    "crop_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pct = 0.2\n",
    "val_split = int(val_pct*len(labels))\n",
    "\n",
    "# TODO: Stratified split\n",
    "#possible_labels = sorted(list(set(labels)))\n",
    "#proportion_in_labels = np.array([sum(labels==i)/len(labels) for i in possible_labels])\n",
    "#val_per_labels = [int(l*val_split) for l in proportion_in_labels]\n",
    "#proportion_in_labels, val_per_labels\n",
    "val_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_weigths = None#1/proportion_in_labels[labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Stratified split\n",
    "#val_idx = np.concatenate([np.random.choice([i for i, l in enumerate(labels) if l==p], c, replace=False) for p,c in zip(possible_labels, val_per_labels)])\n",
    "val_idx = np.random.choice([i for i in range(len(labels))], val_split, replace=False)\n",
    "in_val = np.isin(np.arange(len(labels)),val_idx)\n",
    "in_train = ~in_val\n",
    "train_idx = np.arange(len(labels))[in_train]\n",
    "val_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4#3\n",
    "while (len(images)-val_split)%batch_size==1 or val_split%batch_size==1:\n",
    "    batch_size +=1\n",
    "    print(\"Changing batch size so that no batch has size 1\")\n",
    "batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transforms\n",
    "train_transforms = Compose([ScaleIntensity(), EnsureChannelFirst(), Resize(crop_shapes), RandRotate90()])\n",
    "\n",
    "val_transforms = Compose([ScaleIntensity(), EnsureChannelFirst(), Resize(crop_shapes)])\n",
    "\n",
    "# Define nifti dataset, data loader\n",
    "check_ds = ImageDataset(image_files=images, labels=labels, transform=train_transforms)\n",
    "check_loader = DataLoader(check_ds, batch_size=3, num_workers=2, pin_memory=pin_memory)\n",
    "\n",
    "im, label = monai.utils.misc.first(check_loader)\n",
    "print(type(im), im.shape, label, label.shape)\n",
    "\n",
    "# create a training data loader\n",
    "train_ds = ImageDataset(image_files=images[train_idx].tolist(), labels=labels[train_idx], transform=train_transforms)\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=pin_memory)\n",
    "\n",
    "# create a validation data loader\n",
    "val_ds = ImageDataset(image_files=images[val_idx].tolist(), labels=labels[val_idx], transform=val_transforms)\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size, num_workers=2, pin_memory=pin_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_ds), len(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_loader), len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_fns = {\n",
    "    (s.func.__name__ if isinstance(s, functools.partial) else s.__name__).split(\"_score\")[0]: s\n",
    "    for s in [functools.partial(f1_score, average=\"micro\"), accuracy_score, balanced_accuracy_score]\n",
    "}\n",
    "val_metric = \"balanced_accuracy\"\n",
    "metrics_fns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DenseNet121, CrossEntropyLoss and Adam optimizer\n",
    "model = monai.networks.nets.DenseNet(spatial_dims=3, in_channels=1, out_channels=np.sum(num_labels)).to(device)\n",
    "\n",
    "#loss_function = [torch.nn.CrossEntropyLoss(torch.tensor(1/proportion_in_labels, device=device, dtype=torch.float32)) for _ in label_types]\n",
    "loss_function = [torch.nn.CrossEntropyLoss() for _ in label_types]\n",
    "# loss_function = torch.nn.BCEWithLogitsLoss()  # also works with this data\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), 1e-4)\n",
    "\n",
    "# start a typical PyTorch training\n",
    "val_interval = 2\n",
    "best_metric = -1\n",
    "best_metric_epoch = -1\n",
    "epoch_loss_values = []\n",
    "metric_values = []\n",
    "writer = SummaryWriter(f\"multitask_runs/{datetime.datetime.now():%Y-%m-%d_%H:%M:%S}_{socket.gethostname()}\")\n",
    "max_epochs = 256\n",
    "\n",
    "num_labels_ranges = [0]+num_labels_cumsum\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    try:\n",
    "        print(\"-\" * 10)\n",
    "        print(f\"epoch {epoch + 1}/{max_epochs}\")\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        step = 0\n",
    "\n",
    "        for batch_data in train_loader:\n",
    "            step += 1\n",
    "            b_inputs, b_labels = batch_data[0].to(device), batch_data[1].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(b_inputs)\n",
    "            loss = 0\n",
    "            for l_idx,(loss_fn,loss_w,s,e) in enumerate(zip(loss_function, label_type_weights, num_labels_ranges[:-1],num_labels_ranges[1:])):\n",
    "                loss += loss_fn(outputs[:,s:e], b_labels[:,l_idx])*loss_w\n",
    "            loss /= sum(label_type_weights)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_len = len(train_ds) // train_loader.batch_size\n",
    "            print(f\"{step}/{epoch_len}, train_loss: {loss.item():.4f}\")\n",
    "            writer.add_scalar(\"train_loss\", loss.item(), epoch_len * epoch + step)\n",
    "\n",
    "        epoch_loss /= step\n",
    "        epoch_loss_values.append(epoch_loss)\n",
    "        print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
    "\n",
    "        if (epoch + 1) % val_interval == 0:\n",
    "            model.eval()\n",
    "\n",
    "            num_correct = 0.0\n",
    "            metric_count = 0\n",
    "            v_pred_raw, v_label = get_predicted_values(model, val_loader)\n",
    "            v_pred = np.stack([\n",
    "                    v_pred_raw[:,s:e].argmax(axis=1)\n",
    "                    for (s,e) in zip(num_labels_ranges[:-1],num_labels_ranges[1:])\n",
    "                ],\n",
    "                axis=1,\n",
    "            )\n",
    "\n",
    "            all_metrics_per_label = {\n",
    "                metric: [] for metric in metrics_fns\n",
    "            }\n",
    "            \n",
    "            for l_idx, l in enumerate(label_types):\n",
    "                for metric in all_metrics_per_label:\n",
    "                    metric_value = metrics_fns[metric](v_label[:,l_idx], v_pred[:,l_idx])\n",
    "                    writer.add_scalar(f\"{l} {metric}\",metric_value, epoch + 1)\n",
    "                    all_metrics_per_label[metric].append(metric_value)\n",
    "                cm_counts = get_cm(v_pred[:,l_idx], v_label[:,l_idx], num_labels[l_idx])\n",
    "                cm_pct = cm_counts/cm_counts.sum(axis=1,keepdims=True)\n",
    "                print(cm_pct)\n",
    "                for i in range(num_labels[l_idx]):\n",
    "                    for j in range(num_labels[l_idx]):\n",
    "                        writer.add_scalar(f\"{l} count l{i}_p{j}\",cm_counts[i,j], epoch + 1)\n",
    "                        writer.add_scalar(f\"{l} pct l{i}_p{j}\",cm_pct[i,j], epoch + 1)\n",
    "\n",
    "            metric = np.mean(all_metrics_per_label[val_metric])\n",
    "            metric_values.append(metric)\n",
    "\n",
    "            if metric > best_metric:\n",
    "                best_metric = metric\n",
    "                best_metric_epoch = epoch + 1\n",
    "                torch.save(model.state_dict(), \"multitask_best_metric_model_classification3d_array.pth\")\n",
    "                print(\"saved new best metric model\")\n",
    "\n",
    "            print(f\"Current epoch: {epoch+1} current {val_metric}: {metric:.4f} \")\n",
    "            print(f\"Best {val_metric}: {best_metric:.4f} at epoch {best_metric_epoch}\")\n",
    "            for metric in all_metrics_per_label:\n",
    "                writer.add_scalar(f\"val_{metric}\", np.mean(all_metrics_per_label[metric]), epoch + 1)\n",
    "    except KeyboardInterrupt:\n",
    "        break\n",
    "\n",
    "print(f\"Training completed, best_metric: {best_metric:.4f} at epoch: {best_metric_epoch}\")\n",
    "torch.save(model.state_dict(), \"multitask_last_model_classification3d_array.pth\")\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Occlusion sensitivity\n",
    "One method for trying to visualise why the network made a given prediction is occlusion sensitivity. We occlude part of the image, and see how the probability of a given prediction changes. We then iterate over the image, moving the occluded portion as we go, and in doing so we build up a sensitivity map detailing which areas were the most important in making the decision.\n",
    "\n",
    "#### Bounds\n",
    "If we were to test the occlusion centred on all voxels in our image, we would have to do `torch.prod(im.shape) = 96^3 = ~1e6` predictions. We can use the bounding box to only to the estimations in a region of interest, for example over one slice.\n",
    "\n",
    "To do this, we simply give the bounding box as `(minC,maxC,minD,maxD,minH,maxH,minW,maxW)`. We can use `-1` for any value to use its full extent (`0` and `im.shape-1` for min's and max's, respectively).\n",
    "\n",
    "#### Output\n",
    "The output image in this example will look fairly bad, since our network hasn't been trained for very long. Training for longer should improve the quality of the occlusion map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a validation data loader\n",
    "test_ds = ImageDataset(image_files=test_images, labels=test_labels, transform=val_transforms)\n",
    "test_loader = DataLoader(test_ds, batch_size=2, num_workers=2, pin_memory=pin_memory)\n",
    "\n",
    "train_ds = ImageDataset(image_files=images[train_idx].tolist(), labels=labels[train_idx], transform=train_transforms)\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=pin_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_pred_raw, v_label = get_predicted_values(model, train_loader)\n",
    "v_pred = np.stack([\n",
    "        v_pred_raw[:,s:e].argmax(axis=1)\n",
    "        for (s,e) in zip(num_labels_ranges[:-1],num_labels_ranges[1:])\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "all_metrics_per_label = {\n",
    "    metric: [] for metric in metrics_fns\n",
    "}\n",
    "\n",
    "for l_idx, l in enumerate(label_types):\n",
    "    for metric in all_metrics_per_label:\n",
    "        metric_value = metrics_fns[metric](v_label[:,l_idx], v_pred[:,l_idx])\n",
    "        print(f\"train {l} {metric}\",metric_value)\n",
    "        all_metrics_per_label[metric].append(metric_value)\n",
    "    cm_counts = get_cm(v_pred[:,l_idx], v_label[:,l_idx], num_labels[l_idx])\n",
    "    cm_pct = cm_counts/cm_counts.sum(axis=1,keepdims=True)\n",
    "    print(cm_counts)\n",
    "    print(cm_pct)\n",
    "\n",
    "for metric in all_metrics_per_label:\n",
    "    print(f\"train_{metric}\", np.mean(all_metrics_per_label[metric]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_pred_raw, v_label = get_predicted_values(model, val_loader)\n",
    "v_pred = np.stack([\n",
    "        v_pred_raw[:,s:e].argmax(axis=1)\n",
    "        for (s,e) in zip(num_labels_ranges[:-1],num_labels_ranges[1:])\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "all_metrics_per_label = {\n",
    "    metric: [] for metric in metrics_fns\n",
    "}\n",
    "\n",
    "for l_idx, l in enumerate(label_types):\n",
    "    for metric in all_metrics_per_label:\n",
    "        metric_value = metrics_fns[metric](v_label[:,l_idx], v_pred[:,l_idx])\n",
    "        print(f\"val {l} {metric}\",metric_value)\n",
    "        all_metrics_per_label[metric].append(metric_value)\n",
    "    cm_counts = get_cm(v_pred[:,l_idx], v_label[:,l_idx], num_labels[l_idx])\n",
    "    cm_pct = cm_counts/cm_counts.sum(axis=1,keepdims=True)\n",
    "    print(cm_counts)\n",
    "    print(cm_pct)\n",
    "\n",
    "for metric in all_metrics_per_label:\n",
    "    print(f\"val_{metric}\", np.mean(all_metrics_per_label[metric]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_pred_raw, v_label = get_predicted_values(model, test_loader)\n",
    "v_pred = np.stack([\n",
    "        v_pred_raw[:,s:e].argmax(axis=1)\n",
    "        for (s,e) in zip(num_labels_ranges[:-1],num_labels_ranges[1:])\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "all_metrics_per_label = {\n",
    "    metric: [] for metric in metrics_fns\n",
    "}\n",
    "\n",
    "for l_idx, l in enumerate(label_types):\n",
    "    for metric in all_metrics_per_label:\n",
    "        metric_value = metrics_fns[metric](v_label[:,l_idx], v_pred[:,l_idx])\n",
    "        print(f\"test {l} {metric}\",metric_value)\n",
    "        all_metrics_per_label[metric].append(metric_value)\n",
    "    cm_counts = get_cm(v_pred[:,l_idx], v_label[:,l_idx], num_labels[l_idx])\n",
    "    cm_pct = cm_counts/cm_counts.sum(axis=1,keepdims=True)\n",
    "    print(cm_counts)\n",
    "    print(cm_pct)\n",
    "\n",
    "for metric in all_metrics_per_label:\n",
    "    print(f\"test_{metric}\", np.mean(all_metrics_per_label[metric]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"multitask_best_metric_model_classification3d_array.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_pred_raw, v_label = get_predicted_values(model, train_loader)\n",
    "v_pred = np.stack([\n",
    "        v_pred_raw[:,s:e].argmax(axis=1)\n",
    "        for (s,e) in zip(num_labels_ranges[:-1],num_labels_ranges[1:])\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "all_metrics_per_label = {\n",
    "    metric: [] for metric in metrics_fns\n",
    "}\n",
    "\n",
    "for l_idx, l in enumerate(label_types):\n",
    "    for metric in all_metrics_per_label:\n",
    "        metric_value = metrics_fns[metric](v_label[:,l_idx], v_pred[:,l_idx])\n",
    "        print(f\"train {l} {metric}\",metric_value)\n",
    "        all_metrics_per_label[metric].append(metric_value)\n",
    "    cm_counts = get_cm(v_pred[:,l_idx], v_label[:,l_idx], num_labels[l_idx])\n",
    "    cm_pct = cm_counts/cm_counts.sum(axis=1,keepdims=True)\n",
    "    print(cm_counts)\n",
    "    print(cm_pct)\n",
    "\n",
    "for metric in all_metrics_per_label:\n",
    "    print(f\"train_{metric}\", np.mean(all_metrics_per_label[metric]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_pred_raw, v_label = get_predicted_values(model, val_loader)\n",
    "v_pred = np.stack([\n",
    "        v_pred_raw[:,s:e].argmax(axis=1)\n",
    "        for (s,e) in zip(num_labels_ranges[:-1],num_labels_ranges[1:])\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "all_metrics_per_label = {\n",
    "    metric: [] for metric in metrics_fns\n",
    "}\n",
    "\n",
    "for l_idx, l in enumerate(label_types):\n",
    "    for metric in all_metrics_per_label:\n",
    "        metric_value = metrics_fns[metric](v_label[:,l_idx], v_pred[:,l_idx])\n",
    "        print(f\"val {l} {metric}\",metric_value)\n",
    "        all_metrics_per_label[metric].append(metric_value)\n",
    "    cm_counts = get_cm(v_pred[:,l_idx], v_label[:,l_idx], num_labels[l_idx])\n",
    "    cm_pct = cm_counts/cm_counts.sum(axis=1,keepdims=True)\n",
    "    print(cm_counts)\n",
    "    print(cm_pct)\n",
    "\n",
    "for metric in all_metrics_per_label:\n",
    "    print(f\"val_{metric}\", np.mean(all_metrics_per_label[metric]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_pred_raw, v_label = get_predicted_values(model, test_loader)\n",
    "v_pred = np.stack([\n",
    "        v_pred_raw[:,s:e].argmax(axis=1)\n",
    "        for (s,e) in zip(num_labels_ranges[:-1],num_labels_ranges[1:])\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "all_metrics_per_label = {\n",
    "    metric: [] for metric in metrics_fns\n",
    "}\n",
    "\n",
    "for l_idx, l in enumerate(label_types):\n",
    "    for metric in all_metrics_per_label:\n",
    "        metric_value = metrics_fns[metric](v_label[:,l_idx], v_pred[:,l_idx])\n",
    "        print(f\"test {l} {metric}\",metric_value)\n",
    "        all_metrics_per_label[metric].append(metric_value)\n",
    "    cm_counts = get_cm(v_pred[:,l_idx], v_label[:,l_idx], num_labels[l_idx])\n",
    "    cm_pct = cm_counts/cm_counts.sum(axis=1,keepdims=True)\n",
    "    print(cm_counts)\n",
    "    print(cm_pct)\n",
    "\n",
    "for metric in all_metrics_per_label:\n",
    "    print(f\"test_{metric}\", np.mean(all_metrics_per_label[metric]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_images = np.concatenate([images, test_images])\n",
    "full_labels = np.concatenate([labels, test_labels])\n",
    "\n",
    "full_images.shape, full_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pct = 0.1\n",
    "val_split = int(val_pct*len(full_labels))\n",
    "\n",
    "# TODO: Stratified split\n",
    "#possible_labels = sorted(list(set(labels)))\n",
    "#proportion_in_labels = np.array([sum(labels==i)/len(labels) for i in possible_labels])\n",
    "#val_per_labels = [int(l*val_split) for l in proportion_in_labels]\n",
    "#proportion_in_labels, val_per_labels\n",
    "val_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_weigths = None#1/proportion_in_labels[labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Stratified split\n",
    "#val_idx = np.concatenate([np.random.choice([i for i, l in enumerate(labels) if l==p], c, replace=False) for p,c in zip(possible_labels, val_per_labels)])\n",
    "val_idx = np.random.choice([i for i in range(len(full_labels))], val_split, replace=False)\n",
    "in_val = np.isin(np.arange(len(full_labels)),val_idx)\n",
    "in_train = ~in_val\n",
    "train_idx = np.arange(len(full_labels))[in_train]\n",
    "val_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4#3\n",
    "while (len(full_images)-val_split)%batch_size==1 or val_split%batch_size==1:\n",
    "    batch_size +=1\n",
    "    print(\"Changing batch size so that no batch has size 1\")\n",
    "batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a training data loader\n",
    "train_ds = ImageDataset(image_files=full_images[train_idx].tolist(), labels=full_labels[train_idx], transform=train_transforms)\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=pin_memory)\n",
    "\n",
    "# create a validation data loader\n",
    "val_ds = ImageDataset(image_files=full_images[val_idx].tolist(), labels=full_labels[val_idx], transform=val_transforms)\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size, num_workers=2, pin_memory=pin_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DenseNet121, CrossEntropyLoss and Adam optimizer\n",
    "model = monai.networks.nets.DenseNet(spatial_dims=3, in_channels=1, out_channels=np.sum(num_labels)).to(device)\n",
    "\n",
    "#loss_function = [torch.nn.CrossEntropyLoss(torch.tensor(1/proportion_in_labels, device=device, dtype=torch.float32)) for _ in label_types]\n",
    "loss_function = [torch.nn.CrossEntropyLoss() for _ in label_types]\n",
    "# loss_function = torch.nn.BCEWithLogitsLoss()  # also works with this data\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), 1e-4)\n",
    "\n",
    "# start a typical PyTorch training\n",
    "val_interval = 2\n",
    "best_metric = -1\n",
    "best_metric_epoch = -1\n",
    "epoch_loss_values = []\n",
    "metric_values = []\n",
    "writer = SummaryWriter(f\"full_multitask_runs/{datetime.datetime.now():%Y-%m-%d_%H:%M:%S}_{socket.gethostname()}\")\n",
    "max_epochs = 256\n",
    "\n",
    "num_labels_ranges = [0]+num_labels_cumsum\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    try:\n",
    "        print(\"-\" * 10)\n",
    "        print(f\"epoch {epoch + 1}/{max_epochs}\")\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        step = 0\n",
    "\n",
    "        for batch_data in train_loader:\n",
    "            step += 1\n",
    "            b_inputs, b_labels = batch_data[0].to(device), batch_data[1].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(b_inputs)\n",
    "            loss = 0\n",
    "            for l_idx,(loss_fn,loss_w,s,e) in enumerate(zip(loss_function, label_type_weights, num_labels_ranges[:-1],num_labels_ranges[1:])):\n",
    "                loss += loss_fn(outputs[:,s:e], b_labels[:,l_idx])*loss_w\n",
    "            loss /= sum(label_type_weights)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_len = len(train_ds) // train_loader.batch_size\n",
    "            print(f\"{step}/{epoch_len}, train_loss: {loss.item():.4f}\")\n",
    "            writer.add_scalar(\"train_loss\", loss.item(), epoch_len * epoch + step)\n",
    "\n",
    "        epoch_loss /= step\n",
    "        epoch_loss_values.append(epoch_loss)\n",
    "        print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
    "\n",
    "        if (epoch + 1) % val_interval == 0:\n",
    "            model.eval()\n",
    "\n",
    "            num_correct = 0.0\n",
    "            metric_count = 0\n",
    "            v_pred_raw, v_label = get_predicted_values(model, val_loader)\n",
    "            v_pred = np.stack([\n",
    "                    v_pred_raw[:,s:e].argmax(axis=1)\n",
    "                    for (s,e) in zip(num_labels_ranges[:-1],num_labels_ranges[1:])\n",
    "                ],\n",
    "                axis=1,\n",
    "            )\n",
    "\n",
    "            all_metrics_per_label = {\n",
    "                metric: [] for metric in metrics_fns\n",
    "            }\n",
    "            \n",
    "            for l_idx, l in enumerate(label_types):\n",
    "                for metric in all_metrics_per_label:\n",
    "                    metric_value = metrics_fns[metric](v_label[:,l_idx], v_pred[:,l_idx])\n",
    "                    writer.add_scalar(f\"{l} {metric}\",metric_value, epoch + 1)\n",
    "                    all_metrics_per_label[metric].append(metric_value)\n",
    "                cm_counts = get_cm(v_pred[:,l_idx], v_label[:,l_idx], num_labels[l_idx])\n",
    "                cm_pct = cm_counts/cm_counts.sum(axis=1,keepdims=True)\n",
    "                print(cm_pct)\n",
    "                for i in range(num_labels[l_idx]):\n",
    "                    for j in range(num_labels[l_idx]):\n",
    "                        writer.add_scalar(f\"{l} count l{i}_p{j}\",cm_counts[i,j], epoch + 1)\n",
    "                        writer.add_scalar(f\"{l} pct l{i}_p{j}\",cm_pct[i,j], epoch + 1)\n",
    "\n",
    "            metric = np.mean(all_metrics_per_label[val_metric])\n",
    "            metric_values.append(metric)\n",
    "\n",
    "            if metric > best_metric:\n",
    "                best_metric = metric\n",
    "                best_metric_epoch = epoch + 1\n",
    "                torch.save(model.state_dict(), \"full_multitask_best_metric_model_classification3d_array.pth\")\n",
    "                print(\"saved new best metric model\")\n",
    "\n",
    "            print(f\"Current epoch: {epoch+1} current {val_metric}: {metric:.4f} \")\n",
    "            print(f\"Best {val_metric}: {best_metric:.4f} at epoch {best_metric_epoch}\")\n",
    "            for metric in all_metrics_per_label:\n",
    "                writer.add_scalar(f\"val_{metric}\", np.mean(all_metrics_per_label[metric]), epoch + 1)\n",
    "    except KeyboardInterrupt:\n",
    "        break\n",
    "\n",
    "print(f\"Training completed, best_metric: {best_metric:.4f} at epoch: {best_metric_epoch}\")\n",
    "torch.save(model.state_dict(), \"full_multitask_last_model_classification3d_array.pth\")\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('shade2022')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "3f16b433f0ec686bf0f85bf779465d9d7469a0980a4c6364e7aadd876bd30562"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
