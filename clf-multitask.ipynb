{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3D classification example based on DenseNet\n",
    "\n",
    "This tutorial shows an example of 3D classification task based on DenseNet and array format transforms.\n",
    "\n",
    "Here, the task is given to classify MR images into male/female.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Project-MONAI/tutorials/blob/main/3d_classification/torch/densenet_training_array.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -c \"import monai\" || pip install -q \"monai-weekly[nibabel, tqdm]\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONAI version: 1.0.1\n",
      "Numpy version: 1.23.5\n",
      "Pytorch version: 1.11.0\n",
      "MONAI flags: HAS_EXT = False, USE_COMPILED = False, USE_META_DICT = False\n",
      "MONAI rev id: 8271a193229fe4437026185e218d5b06f7c8ce69\n",
      "MONAI __file__: /home/phcavelar/miniconda3/envs/shade2022/lib/python3.10/site-packages/monai/__init__.py\n",
      "\n",
      "Optional dependencies:\n",
      "Pytorch Ignite version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "Nibabel version: 4.0.2\n",
      "scikit-image version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "Pillow version: 9.2.0\n",
      "Tensorboard version: 2.11.0\n",
      "gdown version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "TorchVision version: 0.12.0\n",
      "tqdm version: 4.64.1\n",
      "lmdb version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "psutil version: 5.9.4\n",
      "pandas version: 1.5.2\n",
      "einops version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "transformers version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "mlflow version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "pynrrd version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "\n",
      "For details about installing the optional dependencies, please visit:\n",
      "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Copyright 2020 MONAI Consortium\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import glob\n",
    "import sys\n",
    "import shutil\n",
    "import tempfile\n",
    "import datetime\n",
    "import socket\n",
    "import functools\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, balanced_accuracy_score, roc_auc_score\n",
    "\n",
    "import monai\n",
    "from monai.apps import download_and_extract\n",
    "from monai.config import print_config\n",
    "from monai.data import DataLoader, ImageDataset\n",
    "from monai.transforms import (\n",
    "    EnsureChannelFirst,\n",
    "    Compose,\n",
    "    RandRotate90,\n",
    "    Resize,\n",
    "    ScaleIntensity,\n",
    ")\n",
    "\n",
    "pin_memory = torch.cuda.is_available()\n",
    "torch.backends.cudnn.benchmark = False #torch.cuda.is_available() # Set this to true if the code fails\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "print_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predicted_values(model, loader):\n",
    "    t_model_outputs = []\n",
    "    t_test_labels = []\n",
    "    for test_data in loader:\n",
    "        test_images, test_labels = test_data[0].to(device), test_data[1].to(device)\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(test_images)\n",
    "            t_model_outputs.append(val_outputs.cpu().detach().numpy())\n",
    "            t_test_labels.append(test_labels.cpu().detach().numpy())\n",
    "    conf_model_outputs = np.concatenate(t_model_outputs)\n",
    "    conf_test_labels = np.concatenate(t_test_labels)\n",
    "    return conf_model_outputs, conf_test_labels\n",
    "\n",
    "def get_cm(conf_model_outputs, conf_test_labels, num_classes):\n",
    "    conf_matrix = np.zeros((num_classes,num_classes))\n",
    "    for i in range(num_classes):\n",
    "        in_class_i = conf_test_labels==i\n",
    "        for j in range(num_classes):\n",
    "            in_class_i_predicted_in_class_j = sum(conf_model_outputs[in_class_i]==j)\n",
    "            conf_matrix[i,j] = in_class_i_predicted_in_class_j\n",
    "    return conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set data directory\n",
    "#directory = os.environ.get(\"MONAI_DATA_DIRECTORY\")\n",
    "#root_dir = tempfile.mkdtemp() if directory is None else directory\n",
    "root_dir = os.path.expanduser(os.path.expandvars(\"~/data/medicaldecathlon/\"))\n",
    "data_dir = os.path.join(root_dir, \"Task10_Colon\")\n",
    "train_dataset_frailty_path = os.path.join(data_dir,\"train_clean.csv\")\n",
    "test_dataset_frailty_path = os.path.join(data_dir,\"test_clean.csv\")\n",
    "print(root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = sorted(\n",
    "    glob.glob(os.path.join(data_dir, \"imagesTr\", \"*.nii.gz\")))\n",
    "train_images[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_paths = sorted(\n",
    "    glob.glob(os.path.join(data_dir, \"imagesTs\", \"*.nii.gz\")))\n",
    "test_image_paths[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_cols = [\"Skeletal Muscle Wasting\",\"Fat Excess\",\"Bone density\",\"Aortic Calcium\",\"Liver fat\",\"Pancreatic fat\",\"Total Score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Skeletal Muscle Wasting</th>\n",
       "      <th>Fat Excess</th>\n",
       "      <th>Bone density</th>\n",
       "      <th>Aortic Calcium</th>\n",
       "      <th>Liver fat</th>\n",
       "      <th>Pancreatic fat</th>\n",
       "      <th>Total Score</th>\n",
       "      <th>Risk Category</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PatientID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>MEDIUM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>LOW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>LOW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>MEDIUM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>LOW</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Skeletal Muscle Wasting  Fat Excess  Bone density  Aortic Calcium  \\\n",
       "PatientID                                                                      \n",
       "1                                1           2             0               1   \n",
       "5                                0           0             0               0   \n",
       "6                                0           1             0               0   \n",
       "7                                1           2             0               0   \n",
       "8                                1           1             0               0   \n",
       "\n",
       "           Liver fat  Pancreatic fat  Total Score Risk Category  \n",
       "PatientID                                                        \n",
       "1                  0               1            5        MEDIUM  \n",
       "5                  1               0            1           LOW  \n",
       "6                  0               0            1           LOW  \n",
       "7                  0               1            4        MEDIUM  \n",
       "8                  0               0            2           LOW  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_labels = pd.read_csv(train_dataset_frailty_path, index_col=\"PatientID\").dropna().astype({col:int for col in int_cols})\n",
    "df_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Skeletal Muscle Wasting</th>\n",
       "      <th>Fat Excess</th>\n",
       "      <th>Bone density</th>\n",
       "      <th>Aortic Calcium</th>\n",
       "      <th>Liver fat</th>\n",
       "      <th>Pancreatic fat</th>\n",
       "      <th>Total Score</th>\n",
       "      <th>Risk Category</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PatientID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>MEDIUM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>MEDIUM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>LOW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>LOW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>MEDIUM</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Skeletal Muscle Wasting  Fat Excess  Bone density  Aortic Calcium  \\\n",
       "PatientID                                                                      \n",
       "3                                1           1             0               1   \n",
       "4                                1           1             0               1   \n",
       "10                               0           0             0               1   \n",
       "13                               0           0             0               0   \n",
       "17                               2           1             0               1   \n",
       "\n",
       "           Liver fat  Pancreatic fat  Total Score Risk Category  \n",
       "PatientID                                                        \n",
       "3                  1               1            5        MEDIUM  \n",
       "4                  1               1            5        MEDIUM  \n",
       "10                 0               1            2           LOW  \n",
       "13                 0               1            1           LOW  \n",
       "17                 0               0            4        MEDIUM  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_labels_test = pd.read_csv(test_dataset_frailty_path, index_col=\"PatientID\").dropna().astype({col:int for col in int_cols})\n",
    "df_labels_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels.loc[df_labels[\"Risk Category\"]==\"LOW\",\"Risk Category\"] = 0\n",
    "df_labels.loc[df_labels[\"Risk Category\"]==\"MEDIUM\",\"Risk Category\"] = 1\n",
    "df_labels.loc[df_labels[\"Risk Category\"]==\"HIGH\",\"Risk Category\"] = 2\n",
    "\n",
    "df_labels_test.loc[df_labels_test[\"Risk Category\"]==\"LOW\",\"Risk Category\"] = 0\n",
    "df_labels_test.loc[df_labels_test[\"Risk Category\"]==\"MEDIUM\",\"Risk Category\"] = 1\n",
    "df_labels_test.loc[df_labels_test[\"Risk Category\"]==\"HIGH\",\"Risk Category\"] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_id_from_filepath(fpath):\n",
    "    return int(os.path.basename(fpath).split(\"_\")[1].split(\".\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Skeletal Muscle Wasting     int64\n",
       "Fat Excess                  int64\n",
       "Bone density                int64\n",
       "Aortic Calcium              int64\n",
       "Liver fat                   int64\n",
       "Pancreatic fat              int64\n",
       "Total Score                 int64\n",
       "Risk Category              object\n",
       "dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_labels.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_types = [\"Skeletal Muscle Wasting\", \"Fat Excess\", \"Bone density\", \"Aortic Calcium\", \"Liver fat\", \"Pancreatic fat\"]\n",
    "label_type_weights = [2,2,1,1,1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_SLICES = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dicts = [\n",
    "    {\"image\": image_name, \"label\": df_labels.loc[patient_id,label_types].astype(int)}\n",
    "    for image_name,patient_id in zip(train_images,map(get_id_from_filepath,train_images))\n",
    "    if patient_id in df_labels.index and nib.load(image_name).get_fdata().shape[2]>=MIN_SLICES\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_dicts = [\n",
    "    {\"image\": image_name, \"label\": df_labels_test.loc[patient_id,label_types].astype(int)}\n",
    "    for image_name,patient_id in zip(test_image_paths,map(get_id_from_filepath,test_image_paths))\n",
    "    if patient_id in df_labels_test.index and nib.load(image_name).get_fdata().shape[2]>=MIN_SLICES\n",
    "]\n",
    "len(test_data_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([4, 4, 2, 4, 2, 4],\n",
       " [4, 8, 10, 14, 16, 20],\n",
       " array([[0, 0, 0, 0, 1, 0],\n",
       "        [0, 1, 0, 0, 0, 0],\n",
       "        [1, 2, 0, 0, 0, 1],\n",
       "        [1, 1, 0, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 0, 1]]),\n",
       " array([[1, 1, 0, 1, 1, 1],\n",
       "        [1, 1, 0, 1, 1, 1],\n",
       "        [0, 0, 0, 1, 0, 1],\n",
       "        [0, 0, 0, 0, 0, 1],\n",
       "        [2, 1, 0, 1, 0, 0]]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# IXI dataset as a demo, downloadable from https://brain-development.org/ixi-dataset/\n",
    "images = np.array([d[\"image\"] for d in data_dicts])\n",
    "# 2 binary labels for gender classification: man or woman\n",
    "labels = np.array([d[\"label\"] for d in data_dicts])\n",
    "\n",
    "test_images = np.array([d[\"image\"] for d in test_data_dicts])\n",
    "# 2 binary labels for gender classification: man or woman\n",
    "test_labels = np.array([d[\"label\"] for d in test_data_dicts])\n",
    "\n",
    "\n",
    "# Represent labels in one-hot format for binary classifier training,\n",
    "# BCEWithLogitsLoss requires target to have same shape as input\n",
    "#labels = torch.nn.functional.one_hot(torch.as_tensor(labels)).float()\n",
    "num_labels = [int(max(labels[:,l]))+1 for l in range(labels.shape[1])] if isinstance(labels, np.ndarray) and len(labels.shape)>1 else int(max(labels))+1\n",
    "num_labels_cumsum = np.cumsum(num_labels).tolist()\n",
    "num_labels, num_labels_cumsum, labels[:5], test_labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([512, 512,  68])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shapes = [nib.load(datapoint[\"image\"]).get_fdata().shape for datapoint in data_dicts]\n",
    "np_shapes = np.stack(shapes)\n",
    "minshapes = np.min(np_shapes, axis=0)\n",
    "minshapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(68, 68, 68)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_crop = 96\n",
    "for newcrop in range(original_crop,2,-1):\n",
    "    if newcrop <= min(minshapes):\n",
    "        crop_shapes = tuple([newcrop]*3)\n",
    "        break\n",
    "crop_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_pct = 0.2\n",
    "val_split = int(val_pct*len(labels))\n",
    "\n",
    "# TODO: Stratified split\n",
    "#possible_labels = sorted(list(set(labels)))\n",
    "#proportion_in_labels = np.array([sum(labels==i)/len(labels) for i in possible_labels])\n",
    "#val_per_labels = [int(l*val_split) for l in proportion_in_labels]\n",
    "#proportion_in_labels, val_per_labels\n",
    "val_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_weigths = None#1/proportion_in_labels[labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 89,   1,  10,   5,  11,  96,  53, 102,  30, 106,  19,  77,  61,\n",
       "        79,  49,  97,  31,  39,  63,  27,  82])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Stratified split\n",
    "#val_idx = np.concatenate([np.random.choice([i for i, l in enumerate(labels) if l==p], c, replace=False) for p,c in zip(possible_labels, val_per_labels)])\n",
    "val_idx = np.random.choice([i for i in range(len(labels))], val_split, replace=False)\n",
    "in_val = np.isin(np.arange(len(labels)),val_idx)\n",
    "in_train = ~in_val\n",
    "train_idx = np.arange(len(labels))[in_train]\n",
    "val_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changing batch size so that no batch has size 1\n",
      "Changing batch size so that no batch has size 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 4#3\n",
    "while (len(images)-val_split)%batch_size==1 or val_split%batch_size==1:\n",
    "    batch_size +=1\n",
    "    print(\"Changing batch size so that no batch has size 1\")\n",
    "batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'monai.data.meta_tensor.MetaTensor'> (3, 1, 68, 68, 68) tensor([[0, 0, 0, 0, 1, 0],\n",
      "        [0, 1, 0, 0, 0, 0],\n",
      "        [1, 2, 0, 0, 0, 1]]) torch.Size([3, 6])\n"
     ]
    }
   ],
   "source": [
    "# Define transforms\n",
    "train_transforms = Compose([ScaleIntensity(), EnsureChannelFirst(), Resize(crop_shapes), RandRotate90()])\n",
    "\n",
    "val_transforms = Compose([ScaleIntensity(), EnsureChannelFirst(), Resize(crop_shapes)])\n",
    "\n",
    "# Define nifti dataset, data loader\n",
    "check_ds = ImageDataset(image_files=images, labels=labels, transform=train_transforms)\n",
    "check_loader = DataLoader(check_ds, batch_size=3, num_workers=2, pin_memory=pin_memory)\n",
    "\n",
    "im, label = monai.utils.misc.first(check_loader)\n",
    "print(type(im), im.shape, label, label.shape)\n",
    "\n",
    "# create a training data loader\n",
    "train_ds = ImageDataset(image_files=images[train_idx].tolist(), labels=labels[train_idx], transform=train_transforms)\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=pin_memory)\n",
    "\n",
    "# create a validation data loader\n",
    "val_ds = ImageDataset(image_files=images[val_idx].tolist(), labels=labels[val_idx], transform=val_transforms)\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size, num_workers=2, pin_memory=pin_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(88, 21)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds), len(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 4)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader), len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1': functools.partial(<function f1_score at 0x7ff0ea940940>, average='micro'),\n",
       " 'accuracy': <function sklearn.metrics._classification.accuracy_score(y_true, y_pred, *, normalize=True, sample_weight=None)>,\n",
       " 'balanced_accuracy': <function sklearn.metrics._classification.balanced_accuracy_score(y_true, y_pred, *, sample_weight=None, adjusted=False)>}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_fns = {\n",
    "    (s.func.__name__ if isinstance(s, functools.partial) else s.__name__).split(\"_score\")[0]: s\n",
    "    for s in [functools.partial(f1_score, average=\"micro\"), accuracy_score, balanced_accuracy_score]\n",
    "}\n",
    "val_metric = \"balanced_accuracy\"\n",
    "metrics_fns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DenseNet121, CrossEntropyLoss and Adam optimizer\n",
    "model = monai.networks.nets.DenseNet(spatial_dims=3, in_channels=1, out_channels=np.sum(num_labels)).to(device)\n",
    "\n",
    "#loss_function = [torch.nn.CrossEntropyLoss(torch.tensor(1/proportion_in_labels, device=device, dtype=torch.float32)) for _ in label_types]\n",
    "loss_function = [torch.nn.CrossEntropyLoss() for _ in label_types]\n",
    "# loss_function = torch.nn.BCEWithLogitsLoss()  # also works with this data\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), 1e-4)\n",
    "\n",
    "# start a typical PyTorch training\n",
    "val_interval = 2\n",
    "best_metric = -1\n",
    "best_metric_epoch = -1\n",
    "epoch_loss_values = []\n",
    "metric_values = []\n",
    "writer = SummaryWriter(f\"multitask_runs/{datetime.datetime.now():%Y-%m-%d_%H:%M:%S}_{socket.gethostname()}\")\n",
    "max_epochs = 256\n",
    "\n",
    "num_labels_ranges = [0]+num_labels_cumsum\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    try:\n",
    "        print(\"-\" * 10)\n",
    "        print(f\"epoch {epoch + 1}/{max_epochs}\")\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        step = 0\n",
    "\n",
    "        for batch_data in train_loader:\n",
    "            step += 1\n",
    "            b_inputs, b_labels = batch_data[0].to(device), batch_data[1].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(b_inputs)\n",
    "            loss = 0\n",
    "            for l_idx,(loss_fn,loss_w,s,e) in enumerate(zip(loss_function, label_type_weights, num_labels_ranges[:-1],num_labels_ranges[1:])):\n",
    "                loss += loss_fn(outputs[:,s:e], b_labels[:,l_idx])*loss_w\n",
    "            loss /= sum(label_type_weights)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_len = len(train_ds) // train_loader.batch_size\n",
    "            print(f\"{step}/{epoch_len}, train_loss: {loss.item():.4f}\")\n",
    "            writer.add_scalar(\"train_loss\", loss.item(), epoch_len * epoch + step)\n",
    "\n",
    "        epoch_loss /= step\n",
    "        epoch_loss_values.append(epoch_loss)\n",
    "        print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
    "\n",
    "        if (epoch + 1) % val_interval == 0:\n",
    "            model.eval()\n",
    "\n",
    "            num_correct = 0.0\n",
    "            metric_count = 0\n",
    "            v_pred_raw, v_label = get_predicted_values(model, val_loader)\n",
    "            v_pred = np.stack([\n",
    "                    v_pred_raw[:,s:e].argmax(axis=1)\n",
    "                    for (s,e) in zip(num_labels_ranges[:-1],num_labels_ranges[1:])\n",
    "                ],\n",
    "                axis=1,\n",
    "            )\n",
    "\n",
    "            all_metrics_per_label = {\n",
    "                metric: [] for metric in metrics_fns\n",
    "            }\n",
    "            \n",
    "            for l_idx, l in enumerate(label_types):\n",
    "                for metric in all_metrics_per_label:\n",
    "                    metric_value = metrics_fns[metric](v_label[:,l_idx], v_pred[:,l_idx])\n",
    "                    writer.add_scalar(f\"{l} {metric}\",metric_value, epoch + 1)\n",
    "                    all_metrics_per_label[metric].append(metric_value)\n",
    "                cm_counts = get_cm(v_pred[:,l_idx], v_label[:,l_idx], num_labels[l_idx])\n",
    "                cm_pct = cm_counts/cm_counts.sum(axis=1,keepdims=True)\n",
    "                print(cm_pct)\n",
    "                for i in range(num_labels[l_idx]):\n",
    "                    for j in range(num_labels[l_idx]):\n",
    "                        writer.add_scalar(f\"{l} count l{i}_p{j}\",cm_counts[i,j], epoch + 1)\n",
    "                        writer.add_scalar(f\"{l} pct l{i}_p{j}\",cm_pct[i,j], epoch + 1)\n",
    "\n",
    "            metric = np.mean(all_metrics_per_label[val_metric])\n",
    "            metric_values.append(metric)\n",
    "\n",
    "            if metric > best_metric:\n",
    "                best_metric = metric\n",
    "                best_metric_epoch = epoch + 1\n",
    "                torch.save(model.state_dict(), \"multitask_best_metric_model_classification3d_array.pth\")\n",
    "                print(\"saved new best metric model\")\n",
    "\n",
    "            print(f\"Current epoch: {epoch+1} current {val_metric}: {metric:.4f} \")\n",
    "            print(f\"Best {val_metric}: {best_metric:.4f} at epoch {best_metric_epoch}\")\n",
    "            for metric in all_metrics_per_label:\n",
    "                writer.add_scalar(f\"val_{metric}\", np.mean(all_metrics_per_label[metric]), epoch + 1)\n",
    "    except KeyboardInterrupt:\n",
    "        break\n",
    "\n",
    "print(f\"Training completed, best_metric: {best_metric:.4f} at epoch: {best_metric_epoch}\")\n",
    "torch.save(model.state_dict(), \"multitask_last_model_classification3d_array.pth\")\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Occlusion sensitivity\n",
    "One method for trying to visualise why the network made a given prediction is occlusion sensitivity. We occlude part of the image, and see how the probability of a given prediction changes. We then iterate over the image, moving the occluded portion as we go, and in doing so we build up a sensitivity map detailing which areas were the most important in making the decision.\n",
    "\n",
    "#### Bounds\n",
    "If we were to test the occlusion centred on all voxels in our image, we would have to do `torch.prod(im.shape) = 96^3 = ~1e6` predictions. We can use the bounding box to only to the estimations in a region of interest, for example over one slice.\n",
    "\n",
    "To do this, we simply give the bounding box as `(minC,maxC,minD,maxD,minH,maxH,minW,maxW)`. We can use `-1` for any value to use its full extent (`0` and `im.shape-1` for min's and max's, respectively).\n",
    "\n",
    "#### Output\n",
    "The output image in this example will look fairly bad, since our network hasn't been trained for very long. Training for longer should improve the quality of the occlusion map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a validation data loader\n",
    "test_ds = ImageDataset(image_files=test_images, labels=test_labels, transform=val_transforms)\n",
    "test_loader = DataLoader(test_ds, batch_size=2, num_workers=2, pin_memory=pin_memory)\n",
    "\n",
    "train_ds = ImageDataset(image_files=images[train_idx].tolist(), labels=labels[train_idx], transform=train_transforms)\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=pin_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Skeletal Muscle Wasting f1 1.0\n",
      "train Skeletal Muscle Wasting accuracy 1.0\n",
      "train Skeletal Muscle Wasting balanced_accuracy 1.0\n",
      "[[31.  0.  0.  0.]\n",
      " [ 0. 37.  0.  0.]\n",
      " [ 0.  0. 19.  0.]\n",
      " [ 0.  0.  0.  1.]]\n",
      "[[1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]]\n",
      "train Fat Excess f1 1.0\n",
      "train Fat Excess accuracy 1.0\n",
      "train Fat Excess balanced_accuracy 1.0\n",
      "[[33.  0.  0.  0.]\n",
      " [ 0. 22.  0.  0.]\n",
      " [ 0.  0. 32.  0.]\n",
      " [ 0.  0.  0.  1.]]\n",
      "[[1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]]\n",
      "train Bone density f1 1.0\n",
      "train Bone density accuracy 1.0\n",
      "train Bone density balanced_accuracy 1.0\n",
      "[[78.  0.]\n",
      " [ 0. 10.]]\n",
      "[[1. 0.]\n",
      " [0. 1.]]\n",
      "train Aortic Calcium f1 1.0\n",
      "train Aortic Calcium accuracy 1.0\n",
      "train Aortic Calcium balanced_accuracy 1.0\n",
      "[[42.  0.  0.  0.]\n",
      " [ 0. 31.  0.  0.]\n",
      " [ 0.  0. 10.  0.]\n",
      " [ 0.  0.  0.  5.]]\n",
      "[[1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]]\n",
      "train Liver fat f1 0.9886363636363636\n",
      "train Liver fat accuracy 0.9886363636363636\n",
      "train Liver fat balanced_accuracy 0.9925373134328358\n",
      "[[66.  1.]\n",
      " [ 0. 21.]]\n",
      "[[0.98507463 0.01492537]\n",
      " [0.         1.        ]]\n",
      "train Pancreatic fat f1 1.0\n",
      "train Pancreatic fat accuracy 1.0\n",
      "train Pancreatic fat balanced_accuracy 1.0\n",
      "[[43.  0.  0.  0.]\n",
      " [ 0. 26.  0.  0.]\n",
      " [ 0.  0. 15.  0.]\n",
      " [ 0.  0.  0.  4.]]\n",
      "[[1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]]\n",
      "train_f1 0.9981060606060606\n",
      "train_accuracy 0.9981060606060606\n",
      "train_balanced_accuracy 0.9987562189054726\n"
     ]
    }
   ],
   "source": [
    "v_pred_raw, v_label = get_predicted_values(model, train_loader)\n",
    "v_pred = np.stack([\n",
    "        v_pred_raw[:,s:e].argmax(axis=1)\n",
    "        for (s,e) in zip(num_labels_ranges[:-1],num_labels_ranges[1:])\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "all_metrics_per_label = {\n",
    "    metric: [] for metric in metrics_fns\n",
    "}\n",
    "\n",
    "for l_idx, l in enumerate(label_types):\n",
    "    for metric in all_metrics_per_label:\n",
    "        metric_value = metrics_fns[metric](v_label[:,l_idx], v_pred[:,l_idx])\n",
    "        print(f\"train {l} {metric}\",metric_value)\n",
    "        all_metrics_per_label[metric].append(metric_value)\n",
    "    cm_counts = get_cm(v_pred[:,l_idx], v_label[:,l_idx], num_labels[l_idx])\n",
    "    cm_pct = cm_counts/cm_counts.sum(axis=1,keepdims=True)\n",
    "    print(cm_counts)\n",
    "    print(cm_pct)\n",
    "\n",
    "for metric in all_metrics_per_label:\n",
    "    print(f\"train_{metric}\", np.mean(all_metrics_per_label[metric]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Skeletal Muscle Wasting f1 0.47619047619047616\n",
      "val Skeletal Muscle Wasting accuracy 0.47619047619047616\n",
      "val Skeletal Muscle Wasting balanced_accuracy 0.48412698412698413\n",
      "[[2. 5. 0. 0.]\n",
      " [0. 4. 2. 0.]\n",
      " [0. 4. 4. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "[[0.28571429 0.71428571 0.         0.        ]\n",
      " [0.         0.66666667 0.33333333 0.        ]\n",
      " [0.         0.5        0.5        0.        ]\n",
      " [       nan        nan        nan        nan]]\n",
      "val Fat Excess f1 0.6190476190476191\n",
      "val Fat Excess accuracy 0.6190476190476191\n",
      "val Fat Excess balanced_accuracy 0.5904761904761905\n",
      "[[4. 3. 0. 0.]\n",
      " [1. 2. 1. 0.]\n",
      " [1. 2. 7. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "[[0.57142857 0.42857143 0.         0.        ]\n",
      " [0.25       0.5        0.25       0.        ]\n",
      " [0.1        0.2        0.7        0.        ]\n",
      " [       nan        nan        nan        nan]]\n",
      "val Bone density f1 0.8571428571428571\n",
      "val Bone density accuracy 0.8571428571428571\n",
      "val Bone density balanced_accuracy 0.5\n",
      "[[18.  0.]\n",
      " [ 3.  0.]]\n",
      "[[1. 0.]\n",
      " [1. 0.]]\n",
      "val Aortic Calcium f1 0.42857142857142855\n",
      "val Aortic Calcium accuracy 0.42857142857142855\n",
      "val Aortic Calcium balanced_accuracy 0.26587301587301587\n",
      "[[7. 1. 1. 0.]\n",
      " [3. 2. 2. 0.]\n",
      " [1. 0. 0. 1.]\n",
      " [1. 1. 1. 0.]]\n",
      "[[0.77777778 0.11111111 0.11111111 0.        ]\n",
      " [0.42857143 0.28571429 0.28571429 0.        ]\n",
      " [0.5        0.         0.         0.5       ]\n",
      " [0.33333333 0.33333333 0.33333333 0.        ]]\n",
      "val Liver fat f1 0.6190476190476191\n",
      "val Liver fat accuracy 0.6190476190476191\n",
      "val Liver fat balanced_accuracy 0.5721153846153846\n",
      "[[10.  3.]\n",
      " [ 5.  3.]]\n",
      "[[0.76923077 0.23076923]\n",
      " [0.625      0.375     ]]\n",
      "val Pancreatic fat f1 0.47619047619047616\n",
      "val Pancreatic fat accuracy 0.47619047619047616\n",
      "val Pancreatic fat balanced_accuracy 0.3833333333333333\n",
      "[[7. 3. 0. 0.]\n",
      " [4. 1. 1. 0.]\n",
      " [0. 1. 2. 0.]\n",
      " [1. 0. 1. 0.]]\n",
      "[[0.7        0.3        0.         0.        ]\n",
      " [0.66666667 0.16666667 0.16666667 0.        ]\n",
      " [0.         0.33333333 0.66666667 0.        ]\n",
      " [0.5        0.         0.5        0.        ]]\n",
      "val_f1 0.5793650793650794\n",
      "val_accuracy 0.5793650793650794\n",
      "val_balanced_accuracy 0.46598748473748475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9284/1428580693.py:19: RuntimeWarning: invalid value encountered in divide\n",
      "  cm_pct = cm_counts/cm_counts.sum(axis=1,keepdims=True)\n",
      "/tmp/ipykernel_9284/1428580693.py:19: RuntimeWarning: invalid value encountered in divide\n",
      "  cm_pct = cm_counts/cm_counts.sum(axis=1,keepdims=True)\n"
     ]
    }
   ],
   "source": [
    "v_pred_raw, v_label = get_predicted_values(model, val_loader)\n",
    "v_pred = np.stack([\n",
    "        v_pred_raw[:,s:e].argmax(axis=1)\n",
    "        for (s,e) in zip(num_labels_ranges[:-1],num_labels_ranges[1:])\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "all_metrics_per_label = {\n",
    "    metric: [] for metric in metrics_fns\n",
    "}\n",
    "\n",
    "for l_idx, l in enumerate(label_types):\n",
    "    for metric in all_metrics_per_label:\n",
    "        metric_value = metrics_fns[metric](v_label[:,l_idx], v_pred[:,l_idx])\n",
    "        print(f\"val {l} {metric}\",metric_value)\n",
    "        all_metrics_per_label[metric].append(metric_value)\n",
    "    cm_counts = get_cm(v_pred[:,l_idx], v_label[:,l_idx], num_labels[l_idx])\n",
    "    cm_pct = cm_counts/cm_counts.sum(axis=1,keepdims=True)\n",
    "    print(cm_counts)\n",
    "    print(cm_pct)\n",
    "\n",
    "for metric in all_metrics_per_label:\n",
    "    print(f\"val_{metric}\", np.mean(all_metrics_per_label[metric]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Skeletal Muscle Wasting f1 0.34545454545454546\n",
      "test Skeletal Muscle Wasting accuracy 0.34545454545454546\n",
      "test Skeletal Muscle Wasting balanced_accuracy 0.26699604743083005\n",
      "[[ 7. 12.  4.  0.]\n",
      " [ 6.  8.  5.  1.]\n",
      " [ 1.  6.  4.  0.]\n",
      " [ 0.  0.  1.  0.]]\n",
      "[[0.30434783 0.52173913 0.17391304 0.        ]\n",
      " [0.3        0.4        0.25       0.05      ]\n",
      " [0.09090909 0.54545455 0.36363636 0.        ]\n",
      " [0.         0.         1.         0.        ]]\n",
      "test Fat Excess f1 0.5454545454545454\n",
      "test Fat Excess accuracy 0.5454545454545454\n",
      "test Fat Excess balanced_accuracy 0.5524720893141946\n",
      "[[15.  5.  5.  0.]\n",
      " [ 3.  8.  8.  0.]\n",
      " [ 1.  3.  7.  0.]\n",
      " [ 0.  0.  0.  0.]]\n",
      "[[0.6        0.2        0.2        0.        ]\n",
      " [0.15789474 0.42105263 0.42105263 0.        ]\n",
      " [0.09090909 0.27272727 0.63636364 0.        ]\n",
      " [       nan        nan        nan        nan]]\n",
      "test Bone density f1 0.8545454545454545\n",
      "test Bone density accuracy 0.8545454545454545\n",
      "test Bone density balanced_accuracy 0.47959183673469385\n",
      "[[47.  2.]\n",
      " [ 6.  0.]]\n",
      "[[0.95918367 0.04081633]\n",
      " [1.         0.        ]]\n",
      "test Aortic Calcium f1 0.4000000000000001\n",
      "test Aortic Calcium accuracy 0.4\n",
      "test Aortic Calcium balanced_accuracy 0.2604166666666667\n",
      "[[15.  8.  6.  1.]\n",
      " [ 7.  6.  1.  2.]\n",
      " [ 1.  4.  1.  0.]\n",
      " [ 0.  1.  2.  0.]]\n",
      "[[0.5        0.26666667 0.2        0.03333333]\n",
      " [0.4375     0.375      0.0625     0.125     ]\n",
      " [0.16666667 0.66666667 0.16666667 0.        ]\n",
      " [0.         0.33333333 0.66666667 0.        ]]\n",
      "test Liver fat f1 0.5636363636363636\n",
      "test Liver fat accuracy 0.5636363636363636\n",
      "test Liver fat balanced_accuracy 0.47115384615384615\n",
      "[[27. 12.]\n",
      " [12.  4.]]\n",
      "[[0.69230769 0.30769231]\n",
      " [0.75       0.25      ]]\n",
      "test Pancreatic fat f1 0.36363636363636365\n",
      "test Pancreatic fat accuracy 0.36363636363636365\n",
      "test Pancreatic fat balanced_accuracy 0.2759502923976608\n",
      "[[12.  4.  2.  1.]\n",
      " [15.  6.  6.  0.]\n",
      " [ 3.  2.  2.  1.]\n",
      " [ 1.  0.  0.  0.]]\n",
      "[[0.63157895 0.21052632 0.10526316 0.05263158]\n",
      " [0.55555556 0.22222222 0.22222222 0.        ]\n",
      " [0.375      0.25       0.25       0.125     ]\n",
      " [1.         0.         0.         0.        ]]\n",
      "test_f1 0.5121212121212121\n",
      "test_accuracy 0.5121212121212121\n",
      "test_balanced_accuracy 0.38443012978298197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9284/1817018252.py:19: RuntimeWarning: invalid value encountered in divide\n",
      "  cm_pct = cm_counts/cm_counts.sum(axis=1,keepdims=True)\n"
     ]
    }
   ],
   "source": [
    "v_pred_raw, v_label = get_predicted_values(model, test_loader)\n",
    "v_pred = np.stack([\n",
    "        v_pred_raw[:,s:e].argmax(axis=1)\n",
    "        for (s,e) in zip(num_labels_ranges[:-1],num_labels_ranges[1:])\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "all_metrics_per_label = {\n",
    "    metric: [] for metric in metrics_fns\n",
    "}\n",
    "\n",
    "for l_idx, l in enumerate(label_types):\n",
    "    for metric in all_metrics_per_label:\n",
    "        metric_value = metrics_fns[metric](v_label[:,l_idx], v_pred[:,l_idx])\n",
    "        print(f\"test {l} {metric}\",metric_value)\n",
    "        all_metrics_per_label[metric].append(metric_value)\n",
    "    cm_counts = get_cm(v_pred[:,l_idx], v_label[:,l_idx], num_labels[l_idx])\n",
    "    cm_pct = cm_counts/cm_counts.sum(axis=1,keepdims=True)\n",
    "    print(cm_counts)\n",
    "    print(cm_pct)\n",
    "\n",
    "for metric in all_metrics_per_label:\n",
    "    print(f\"test_{metric}\", np.mean(all_metrics_per_label[metric]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"multitask_best_metric_model_classification3d_array.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Skeletal Muscle Wasting f1 1.0\n",
      "train Skeletal Muscle Wasting accuracy 1.0\n",
      "train Skeletal Muscle Wasting balanced_accuracy 1.0\n",
      "[[31.  0.  0.  0.]\n",
      " [ 0. 37.  0.  0.]\n",
      " [ 0.  0. 19.  0.]\n",
      " [ 0.  0.  0.  1.]]\n",
      "[[1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]]\n",
      "train Fat Excess f1 1.0\n",
      "train Fat Excess accuracy 1.0\n",
      "train Fat Excess balanced_accuracy 1.0\n",
      "[[33.  0.  0.  0.]\n",
      " [ 0. 22.  0.  0.]\n",
      " [ 0.  0. 32.  0.]\n",
      " [ 0.  0.  0.  1.]]\n",
      "[[1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]]\n",
      "train Bone density f1 1.0\n",
      "train Bone density accuracy 1.0\n",
      "train Bone density balanced_accuracy 1.0\n",
      "[[78.  0.]\n",
      " [ 0. 10.]]\n",
      "[[1. 0.]\n",
      " [0. 1.]]\n",
      "train Aortic Calcium f1 1.0\n",
      "train Aortic Calcium accuracy 1.0\n",
      "train Aortic Calcium balanced_accuracy 1.0\n",
      "[[42.  0.  0.  0.]\n",
      " [ 0. 31.  0.  0.]\n",
      " [ 0.  0. 10.  0.]\n",
      " [ 0.  0.  0.  5.]]\n",
      "[[1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]]\n",
      "train Liver fat f1 1.0\n",
      "train Liver fat accuracy 1.0\n",
      "train Liver fat balanced_accuracy 1.0\n",
      "[[67.  0.]\n",
      " [ 0. 21.]]\n",
      "[[1. 0.]\n",
      " [0. 1.]]\n",
      "train Pancreatic fat f1 1.0\n",
      "train Pancreatic fat accuracy 1.0\n",
      "train Pancreatic fat balanced_accuracy 1.0\n",
      "[[43.  0.  0.  0.]\n",
      " [ 0. 26.  0.  0.]\n",
      " [ 0.  0. 15.  0.]\n",
      " [ 0.  0.  0.  4.]]\n",
      "[[1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]]\n",
      "train_f1 1.0\n",
      "train_accuracy 1.0\n",
      "train_balanced_accuracy 1.0\n"
     ]
    }
   ],
   "source": [
    "v_pred_raw, v_label = get_predicted_values(model, train_loader)\n",
    "v_pred = np.stack([\n",
    "        v_pred_raw[:,s:e].argmax(axis=1)\n",
    "        for (s,e) in zip(num_labels_ranges[:-1],num_labels_ranges[1:])\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "all_metrics_per_label = {\n",
    "    metric: [] for metric in metrics_fns\n",
    "}\n",
    "\n",
    "for l_idx, l in enumerate(label_types):\n",
    "    for metric in all_metrics_per_label:\n",
    "        metric_value = metrics_fns[metric](v_label[:,l_idx], v_pred[:,l_idx])\n",
    "        print(f\"train {l} {metric}\",metric_value)\n",
    "        all_metrics_per_label[metric].append(metric_value)\n",
    "    cm_counts = get_cm(v_pred[:,l_idx], v_label[:,l_idx], num_labels[l_idx])\n",
    "    cm_pct = cm_counts/cm_counts.sum(axis=1,keepdims=True)\n",
    "    print(cm_counts)\n",
    "    print(cm_pct)\n",
    "\n",
    "for metric in all_metrics_per_label:\n",
    "    print(f\"train_{metric}\", np.mean(all_metrics_per_label[metric]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff193ec2050>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/phcavelar/miniconda3/envs/shade2022/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1358, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/phcavelar/miniconda3/envs/shade2022/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1341, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/home/phcavelar/miniconda3/envs/shade2022/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff193ec2050>\n",
      "    Traceback (most recent call last):\n",
      "assert self._parent_pid == os.getpid(), 'can only test a child process'  File \"/home/phcavelar/miniconda3/envs/shade2022/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1358, in __del__\n",
      "\n",
      "    AssertionErrorself._shutdown_workers(): \n",
      "can only test a child process  File \"/home/phcavelar/miniconda3/envs/shade2022/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1341, in _shutdown_workers\n",
      "\n",
      "    if w.is_alive():\n",
      "  File \"/home/phcavelar/miniconda3/envs/shade2022/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Skeletal Muscle Wasting f1 0.42857142857142855\n",
      "val Skeletal Muscle Wasting accuracy 0.42857142857142855\n",
      "val Skeletal Muscle Wasting balanced_accuracy 0.4365079365079365\n",
      "[[1. 5. 1. 0.]\n",
      " [0. 4. 2. 0.]\n",
      " [0. 4. 4. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "[[0.14285714 0.71428571 0.14285714 0.        ]\n",
      " [0.         0.66666667 0.33333333 0.        ]\n",
      " [0.         0.5        0.5        0.        ]\n",
      " [       nan        nan        nan        nan]]\n",
      "val Fat Excess f1 0.7619047619047619\n",
      "val Fat Excess accuracy 0.7619047619047619\n",
      "val Fat Excess balanced_accuracy 0.769047619047619\n",
      "[[6. 1. 0. 0.]\n",
      " [1. 3. 0. 0.]\n",
      " [0. 3. 7. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "[[0.85714286 0.14285714 0.         0.        ]\n",
      " [0.25       0.75       0.         0.        ]\n",
      " [0.         0.3        0.7        0.        ]\n",
      " [       nan        nan        nan        nan]]\n",
      "val Bone density f1 0.8571428571428571\n",
      "val Bone density accuracy 0.8571428571428571\n",
      "val Bone density balanced_accuracy 0.5\n",
      "[[18.  0.]\n",
      " [ 3.  0.]]\n",
      "[[1. 0.]\n",
      " [1. 0.]]\n",
      "val Aortic Calcium f1 0.5714285714285714\n",
      "val Aortic Calcium accuracy 0.5714285714285714\n",
      "val Aortic Calcium balanced_accuracy 0.4623015873015873\n",
      "[[7. 2. 0. 0.]\n",
      " [3. 4. 0. 0.]\n",
      " [0. 1. 1. 0.]\n",
      " [1. 1. 1. 0.]]\n",
      "[[0.77777778 0.22222222 0.         0.        ]\n",
      " [0.42857143 0.57142857 0.         0.        ]\n",
      " [0.         0.5        0.5        0.        ]\n",
      " [0.33333333 0.33333333 0.33333333 0.        ]]\n",
      "val Liver fat f1 0.5714285714285714\n",
      "val Liver fat accuracy 0.5714285714285714\n",
      "val Liver fat balanced_accuracy 0.4855769230769231\n",
      "[[11.  2.]\n",
      " [ 7.  1.]]\n",
      "[[0.84615385 0.15384615]\n",
      " [0.875      0.125     ]]\n",
      "val Pancreatic fat f1 0.5238095238095238\n",
      "val Pancreatic fat accuracy 0.5238095238095238\n",
      "val Pancreatic fat balanced_accuracy 0.45\n",
      "[[8. 1. 0. 1.]\n",
      " [4. 0. 2. 0.]\n",
      " [0. 0. 3. 0.]\n",
      " [0. 0. 2. 0.]]\n",
      "[[0.8        0.1        0.         0.1       ]\n",
      " [0.66666667 0.         0.33333333 0.        ]\n",
      " [0.         0.         1.         0.        ]\n",
      " [0.         0.         1.         0.        ]]\n",
      "val_f1 0.619047619047619\n",
      "val_accuracy 0.619047619047619\n",
      "val_balanced_accuracy 0.5172390109890109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9284/1428580693.py:19: RuntimeWarning: invalid value encountered in divide\n",
      "  cm_pct = cm_counts/cm_counts.sum(axis=1,keepdims=True)\n",
      "/tmp/ipykernel_9284/1428580693.py:19: RuntimeWarning: invalid value encountered in divide\n",
      "  cm_pct = cm_counts/cm_counts.sum(axis=1,keepdims=True)\n"
     ]
    }
   ],
   "source": [
    "v_pred_raw, v_label = get_predicted_values(model, val_loader)\n",
    "v_pred = np.stack([\n",
    "        v_pred_raw[:,s:e].argmax(axis=1)\n",
    "        for (s,e) in zip(num_labels_ranges[:-1],num_labels_ranges[1:])\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "all_metrics_per_label = {\n",
    "    metric: [] for metric in metrics_fns\n",
    "}\n",
    "\n",
    "for l_idx, l in enumerate(label_types):\n",
    "    for metric in all_metrics_per_label:\n",
    "        metric_value = metrics_fns[metric](v_label[:,l_idx], v_pred[:,l_idx])\n",
    "        print(f\"val {l} {metric}\",metric_value)\n",
    "        all_metrics_per_label[metric].append(metric_value)\n",
    "    cm_counts = get_cm(v_pred[:,l_idx], v_label[:,l_idx], num_labels[l_idx])\n",
    "    cm_pct = cm_counts/cm_counts.sum(axis=1,keepdims=True)\n",
    "    print(cm_counts)\n",
    "    print(cm_pct)\n",
    "\n",
    "for metric in all_metrics_per_label:\n",
    "    print(f\"val_{metric}\", np.mean(all_metrics_per_label[metric]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Skeletal Muscle Wasting f1 0.34545454545454546\n",
      "test Skeletal Muscle Wasting accuracy 0.34545454545454546\n",
      "test Skeletal Muscle Wasting balanced_accuracy 0.27188735177865614\n",
      "[[ 4. 15.  4.  0.]\n",
      " [ 3. 11.  6.  0.]\n",
      " [ 1.  6.  4.  0.]\n",
      " [ 0.  0.  1.  0.]]\n",
      "[[0.17391304 0.65217391 0.17391304 0.        ]\n",
      " [0.15       0.55       0.3        0.        ]\n",
      " [0.09090909 0.54545455 0.36363636 0.        ]\n",
      " [0.         0.         1.         0.        ]]\n",
      "test Fat Excess f1 0.509090909090909\n",
      "test Fat Excess accuracy 0.509090909090909\n",
      "test Fat Excess balanced_accuracy 0.525933014354067\n",
      "[[16.  5.  4.  0.]\n",
      " [ 3.  4. 12.  0.]\n",
      " [ 2.  1.  8.  0.]\n",
      " [ 0.  0.  0.  0.]]\n",
      "[[0.64       0.2        0.16       0.        ]\n",
      " [0.15789474 0.21052632 0.63157895 0.        ]\n",
      " [0.18181818 0.09090909 0.72727273 0.        ]\n",
      " [       nan        nan        nan        nan]]\n",
      "test Bone density f1 0.7818181818181819\n",
      "test Bone density accuracy 0.7818181818181819\n",
      "test Bone density balanced_accuracy 0.5119047619047619\n",
      "[[42.  7.]\n",
      " [ 5.  1.]]\n",
      "[[0.85714286 0.14285714]\n",
      " [0.83333333 0.16666667]]\n",
      "test Aortic Calcium f1 0.4000000000000001\n",
      "test Aortic Calcium accuracy 0.4\n",
      "test Aortic Calcium balanced_accuracy 0.30104166666666665\n",
      "[[13. 12.  4.  1.]\n",
      " [ 6.  7.  2.  1.]\n",
      " [ 1.  3.  2.  0.]\n",
      " [ 0.  2.  1.  0.]]\n",
      "[[0.43333333 0.4        0.13333333 0.03333333]\n",
      " [0.375      0.4375     0.125      0.0625    ]\n",
      " [0.16666667 0.5        0.33333333 0.        ]\n",
      " [0.         0.66666667 0.33333333 0.        ]]\n",
      "test Liver fat f1 0.6\n",
      "test Liver fat accuracy 0.6\n",
      "test Liver fat balanced_accuracy 0.4599358974358974\n",
      "[[31.  8.]\n",
      " [14.  2.]]\n",
      "[[0.79487179 0.20512821]\n",
      " [0.875      0.125     ]]\n",
      "test Pancreatic fat f1 0.41818181818181815\n",
      "test Pancreatic fat accuracy 0.41818181818181815\n",
      "test Pancreatic fat balanced_accuracy 0.31152534113060426\n",
      "[[14.  3.  2.  0.]\n",
      " [14.  7.  6.  0.]\n",
      " [ 5.  0.  2.  1.]\n",
      " [ 1.  0.  0.  0.]]\n",
      "[[0.73684211 0.15789474 0.10526316 0.        ]\n",
      " [0.51851852 0.25925926 0.22222222 0.        ]\n",
      " [0.625      0.         0.25       0.125     ]\n",
      " [1.         0.         0.         0.        ]]\n",
      "test_f1 0.509090909090909\n",
      "test_accuracy 0.509090909090909\n",
      "test_balanced_accuracy 0.3970380055451089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9284/1817018252.py:19: RuntimeWarning: invalid value encountered in divide\n",
      "  cm_pct = cm_counts/cm_counts.sum(axis=1,keepdims=True)\n"
     ]
    }
   ],
   "source": [
    "v_pred_raw, v_label = get_predicted_values(model, test_loader)\n",
    "v_pred = np.stack([\n",
    "        v_pred_raw[:,s:e].argmax(axis=1)\n",
    "        for (s,e) in zip(num_labels_ranges[:-1],num_labels_ranges[1:])\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "all_metrics_per_label = {\n",
    "    metric: [] for metric in metrics_fns\n",
    "}\n",
    "\n",
    "for l_idx, l in enumerate(label_types):\n",
    "    for metric in all_metrics_per_label:\n",
    "        metric_value = metrics_fns[metric](v_label[:,l_idx], v_pred[:,l_idx])\n",
    "        print(f\"test {l} {metric}\",metric_value)\n",
    "        all_metrics_per_label[metric].append(metric_value)\n",
    "    cm_counts = get_cm(v_pred[:,l_idx], v_label[:,l_idx], num_labels[l_idx])\n",
    "    cm_pct = cm_counts/cm_counts.sum(axis=1,keepdims=True)\n",
    "    print(cm_counts)\n",
    "    print(cm_pct)\n",
    "\n",
    "for metric in all_metrics_per_label:\n",
    "    print(f\"test_{metric}\", np.mean(all_metrics_per_label[metric]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup data directory\n",
    "\n",
    "Remove directory if a temporary was used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('shade2022')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "3f16b433f0ec686bf0f85bf779465d9d7469a0980a4c6364e7aadd876bd30562"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
