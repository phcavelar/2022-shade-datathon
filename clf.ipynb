{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3D classification example based on DenseNet\n",
    "\n",
    "This tutorial shows an example of 3D classification task based on DenseNet and array format transforms.\n",
    "\n",
    "Here, the task is given to classify MR images into male/female.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Project-MONAI/tutorials/blob/main/3d_classification/torch/densenet_training_array.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -c \"import monai\" || pip install -q \"monai-weekly[nibabel, tqdm]\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2020 MONAI Consortium\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import glob\n",
    "import sys\n",
    "import shutil\n",
    "import tempfile\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "#from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "\n",
    "import monai\n",
    "from monai.apps import download_and_extract\n",
    "from monai.config import print_config\n",
    "from monai.data import DataLoader, ImageDataset\n",
    "from monai.transforms import (\n",
    "    EnsureChannelFirst,\n",
    "    Compose,\n",
    "    RandRotate90,\n",
    "    Resize,\n",
    "    ScaleIntensity,\n",
    ")\n",
    "\n",
    "pin_memory = torch.cuda.is_available()\n",
    "torch.backends.cudnn.benchmark = False #torch.cuda.is_available() # Set this to true if the code fails\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "print_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set data directory\n",
    "#directory = os.environ.get(\"MONAI_DATA_DIRECTORY\")\n",
    "#root_dir = tempfile.mkdtemp() if directory is None else directory\n",
    "root_dir = os.path.expanduser(os.path.expandvars(\"~/data/medicaldecathlon/\"))\n",
    "data_dir = os.path.join(root_dir, \"Task10_Colon\")\n",
    "train_dataset_frailty_path = os.path.join(data_dir,\"train_clean.csv\")\n",
    "test_dataset_frailty_path = os.path.join(data_dir,\"test_clean.csv\")\n",
    "print(root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = sorted(\n",
    "    glob.glob(os.path.join(data_dir, \"imagesTr\", \"*.nii.gz\")))\n",
    "train_images[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_paths = sorted(\n",
    "    glob.glob(os.path.join(data_dir, \"imagesTs\", \"*.nii.gz\")))\n",
    "test_image_paths[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels = pd.read_csv(train_dataset_frailty_path, index_col=\"PatientID\")\n",
    "df_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels_test = pd.read_csv(test_dataset_frailty_path, index_col=\"PatientID\")\n",
    "df_labels_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels.loc[df_labels[\"Risk Category\"]==\"LOW\",\"Risk Category\"] = 0\n",
    "df_labels.loc[df_labels[\"Risk Category\"]==\"MEDIUM\",\"Risk Category\"] = 1\n",
    "df_labels.loc[df_labels[\"Risk Category\"]==\"HIGH\",\"Risk Category\"] = 2\n",
    "\n",
    "df_labels_test.loc[df_labels_test[\"Risk Category\"]==\"LOW\",\"Risk Category\"] = 0\n",
    "df_labels_test.loc[df_labels_test[\"Risk Category\"]==\"MEDIUM\",\"Risk Category\"] = 1\n",
    "df_labels_test.loc[df_labels_test[\"Risk Category\"]==\"HIGH\",\"Risk Category\"] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_id_from_filepath(fpath):\n",
    "    return int(os.path.basename(fpath).split(\"_\")[1].split(\".\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_split = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dicts = [\n",
    "    {\"image\": image_name, \"label\": df_labels.loc[patient_id,\"Risk Category\"]}\n",
    "    for image_name,patient_id in zip(train_images,map(get_id_from_filepath,train_images))\n",
    "    if patient_id in df_labels.index\n",
    "]\n",
    "train_files, val_files = data_dicts[:-val_split], data_dicts[-val_split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_dicts = [\n",
    "    {\"image\": image_name, \"label\": df_labels_test.loc[patient_id,\"Risk Category\"]}\n",
    "    for image_name,patient_id in zip(test_image_paths,map(get_id_from_filepath,test_image_paths))\n",
    "    if patient_id in df_labels_test.index\n",
    "]\n",
    "len(test_data_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IXI dataset as a demo, downloadable from https://brain-development.org/ixi-dataset/\n",
    "images = [d[\"image\"] for d in data_dicts]\n",
    "# 2 binary labels for gender classification: man or woman\n",
    "labels = [d[\"label\"] for d in data_dicts]\n",
    "\n",
    "test_images = [d[\"image\"] for d in test_data_dicts]\n",
    "# 2 binary labels for gender classification: man or woman\n",
    "test_labels = [d[\"label\"] for d in test_data_dicts]\n",
    "\n",
    "\n",
    "# Represent labels in one-hot format for binary classifier training,\n",
    "# BCEWithLogitsLoss requires target to have same shape as input\n",
    "#labels = torch.nn.functional.one_hot(torch.as_tensor(labels)).float()\n",
    "num_labels = int(max(labels))+1\n",
    "num_labels, labels[:5], test_labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes = [nib.load(datapoint[\"image\"]).get_fdata().shape for datapoint in data_dicts]\n",
    "np_shapes = np.stack(shapes)\n",
    "minshapes = np.min(np_shapes, axis=0)\n",
    "minshapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_crop = 96\n",
    "for divisor in range(1,96//2+1):\n",
    "    if original_crop//divisor <= min(minshapes):\n",
    "        crop_shapes = tuple([original_crop//divisor]*3)\n",
    "        break\n",
    "crop_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4#3\n",
    "while (len(images)-val_split)%batch_size==1 or val_split%batch_size==1:\n",
    "    batch_size +=1\n",
    "    print(\"Changing batch size so that no batch has size 1\")\n",
    "batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transforms\n",
    "train_transforms = Compose([ScaleIntensity(), EnsureChannelFirst(), Resize(crop_shapes), RandRotate90()])\n",
    "\n",
    "val_transforms = Compose([ScaleIntensity(), EnsureChannelFirst(), Resize(crop_shapes)])\n",
    "\n",
    "# Define nifti dataset, data loader\n",
    "check_ds = ImageDataset(image_files=images, labels=labels, transform=train_transforms)\n",
    "check_loader = DataLoader(check_ds, batch_size=3, num_workers=2, pin_memory=pin_memory)\n",
    "\n",
    "im, label = monai.utils.misc.first(check_loader)\n",
    "print(type(im), im.shape, label, label.shape)\n",
    "\n",
    "# create a training data loader\n",
    "train_ds = ImageDataset(image_files=images[:-val_split], labels=labels[:-val_split], transform=train_transforms)\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=pin_memory)\n",
    "\n",
    "# create a validation data loader\n",
    "val_ds = ImageDataset(image_files=images[-val_split:], labels=labels[-val_split:], transform=val_transforms)\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size, num_workers=2, pin_memory=pin_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DenseNet121, CrossEntropyLoss and Adam optimizer\n",
    "model = monai.networks.nets.DenseNet(spatial_dims=3, in_channels=1, out_channels=num_labels).to(device)\n",
    "\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "# loss_function = torch.nn.BCEWithLogitsLoss()  # also works with this data\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), 1e-4)\n",
    "\n",
    "# start a typical PyTorch training\n",
    "val_interval = 2\n",
    "best_metric = -1\n",
    "best_metric_epoch = -1\n",
    "epoch_loss_values = []\n",
    "metric_values = []\n",
    "#writer = SummaryWriter()\n",
    "max_epochs = 256\n",
    "\n",
    "once = True\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    try:\n",
    "        print(\"-\" * 10)\n",
    "        print(f\"epoch {epoch + 1}/{max_epochs}\")\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        step = 0\n",
    "\n",
    "        for batch_data in train_loader:\n",
    "            step += 1\n",
    "            inputs, labels = batch_data[0].to(device), batch_data[1].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            print(inputs.shape)\n",
    "            outputs = model(inputs)\n",
    "            if once:\n",
    "                print(outputs.shape, labels.shape, loss_function(outputs, labels))\n",
    "                once = False\n",
    "            loss = loss_function(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_len = len(train_ds) // train_loader.batch_size\n",
    "            print(f\"{step}/{epoch_len}, train_loss: {loss.item():.4f}\")\n",
    "            #writer.add_scalar(\"train_loss\", loss.item(), epoch_len * epoch + step)\n",
    "\n",
    "        epoch_loss /= step\n",
    "        epoch_loss_values.append(epoch_loss)\n",
    "        print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
    "\n",
    "        if (epoch + 1) % val_interval == 0:\n",
    "            model.eval()\n",
    "\n",
    "            num_correct = 0.0\n",
    "            metric_count = 0\n",
    "            for val_data in val_loader:\n",
    "                val_images, val_labels = val_data[0].to(device), val_data[1].to(device)\n",
    "                with torch.no_grad():\n",
    "                    val_outputs = model(val_images)\n",
    "                    value = torch.eq(val_outputs.argmax(dim=1), val_labels)\n",
    "                    metric_count += len(value)\n",
    "                    num_correct += value.sum().item()\n",
    "\n",
    "            metric = num_correct / metric_count\n",
    "            metric_values.append(metric)\n",
    "\n",
    "            if metric > best_metric:\n",
    "                best_metric = metric\n",
    "                best_metric_epoch = epoch + 1\n",
    "                torch.save(model.state_dict(), \"best_metric_model_classification3d_array.pth\")\n",
    "                print(\"saved new best metric model\")\n",
    "\n",
    "            print(f\"Current epoch: {epoch+1} current accuracy: {metric:.4f} \")\n",
    "            print(f\"Best accuracy: {best_metric:.4f} at epoch {best_metric_epoch}\")\n",
    "            #writer.add_scalar(\"val_accuracy\", metric, epoch + 1)\n",
    "    except KeyboardInterrupt: \n",
    "        break\n",
    "\n",
    "print(f\"Training completed, best_metric: {best_metric:.4f} at epoch: {best_metric_epoch}\")\n",
    "#writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Occlusion sensitivity\n",
    "One method for trying to visualise why the network made a given prediction is occlusion sensitivity. We occlude part of the image, and see how the probability of a given prediction changes. We then iterate over the image, moving the occluded portion as we go, and in doing so we build up a sensitivity map detailing which areas were the most important in making the decision.\n",
    "\n",
    "#### Bounds\n",
    "If we were to test the occlusion centred on all voxels in our image, we would have to do `torch.prod(im.shape) = 96^3 = ~1e6` predictions. We can use the bounding box to only to the estimations in a region of interest, for example over one slice.\n",
    "\n",
    "To do this, we simply give the bounding box as `(minC,maxC,minD,maxD,minH,maxH,minW,maxW)`. We can use `-1` for any value to use its full extent (`0` and `im.shape-1` for min's and max's, respectively).\n",
    "\n",
    "#### Output\n",
    "The output image in this example will look fairly bad, since our network hasn't been trained for very long. Training for longer should improve the quality of the occlusion map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a validation data loader\n",
    "test_ds = ImageDataset(image_files=test_images, labels=test_labels, transform=val_transforms)\n",
    "test_loader = DataLoader(test_ds, batch_size=2, num_workers=2, pin_memory=torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_correct = 0.0\n",
    "metric_count = 0\n",
    "for test_data in test_loader:\n",
    "    test_images, test_labels = test_data[0].to(device), test_data[1].to(device)\n",
    "    with torch.no_grad():\n",
    "        val_outputs = model(test_images)\n",
    "        value = torch.eq(val_outputs.argmax(dim=1), test_labels)\n",
    "        metric_count += len(value)\n",
    "        num_correct += value.sum().item()\n",
    "\n",
    "metric = num_correct / metric_count\n",
    "metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occ_loader = DataLoader(test_ds, batch_size=1, num_workers=2, pin_memory=pin_memory)\n",
    "itera = iter(occ_loader)\n",
    "\n",
    "\n",
    "def get_next_im():\n",
    "    test_data = next(itera)\n",
    "    return test_data[0].to(device), test_data[1].unsqueeze(0).to(device)\n",
    "\n",
    "\n",
    "def plot_occlusion_heatmap(im, heatmap):\n",
    "    plt.subplots(1, 2)\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(np.squeeze(im.cpu()))\n",
    "    plt.colorbar()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(heatmap)\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a random image and its corresponding label\n",
    "img, label = get_next_im()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the occlusion sensitivity map\n",
    "occ_sens = monai.visualize.OcclusionSensitivity(nn_module=model, mask_size=12, n_batch=10, stride=12)\n",
    "# Only get a single slice to save time.\n",
    "# For the other dimensions (channel, width, height), use\n",
    "# -1 to use 0 and img.shape[x]-1 for min and max, respectively\n",
    "depth_slice = img.shape[2] // 2\n",
    "occ_sens_b_box = [depth_slice-1, depth_slice, -1, -1, -1, -1]\n",
    "\n",
    "occ_result, _ = occ_sens(x=img, b_box=occ_sens_b_box)\n",
    "occ_result = occ_result[0, label.argmax().item()][None]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(25, 15), facecolor=\"white\")\n",
    "\n",
    "for i, im in enumerate([img[:, :, depth_slice, ...], occ_result]):\n",
    "    cmap = \"gray\" if i == 0 else \"jet\"\n",
    "    ax = axes[i]\n",
    "    im_show = ax.imshow(np.squeeze(im[0][0].detach().cpu()), cmap=cmap)\n",
    "    ax.axis(\"off\")\n",
    "    fig.colorbar(im_show, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup data directory\n",
    "\n",
    "Remove directory if a temporary was used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if directory is None:\n",
    "    shutil.rmtree(root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('shade2022')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "3f16b433f0ec686bf0f85bf779465d9d7469a0980a4c6364e7aadd876bd30562"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
